{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fcWG5z6flWDo",
    "outputId": "bbf30fc9-4019-49fc-9795-a7d10e351e2b"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/ImageBind\n",
    "%cd ImageBind\n",
    "!pip install .\n",
    "\n",
    "from imagebind import data\n",
    "import torch\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYNxFszmpvQF",
    "outputId": "aafde94c-b4ff-49dd-eef9-6ee8f0810fe9"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzG8H4BDrBJy"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_row_diagonal_accuracy(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of rows where the diagonal element is the highest value.\n",
    "\n",
    "    Parameters:\n",
    "    confusion_matrix : array-like\n",
    "        A square confusion matrix where rows represent actual classes\n",
    "        and columns represent predicted classes\n",
    "\n",
    "    Returns:\n",
    "    float: Percentage (0-1) of rows where diagonal element is highest\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it isn't already\n",
    "    conf_matrix = np.array(confusion_matrix)\n",
    "\n",
    "    # Verify it's a square matrix\n",
    "    if conf_matrix.shape[0] != conf_matrix.shape[1]:\n",
    "        raise ValueError(\"Confusion matrix must be square\")\n",
    "\n",
    "    correct_rows = 0\n",
    "    total_rows = conf_matrix.shape[0]\n",
    "\n",
    "    # For each row, check if diagonal element is the maximum\n",
    "    for i in range(total_rows):\n",
    "        if conf_matrix[i, i] >= np.max(conf_matrix[i]):  # >= handles case where diagonal equals another value\n",
    "            correct_rows += 1\n",
    "\n",
    "    return correct_rows / total_rows\n",
    "\n",
    "def load_descriptions():\n",
    "  \"\"\"\n",
    "  Load descriptions from descriptions.txt into a list where the index matches the description.\n",
    "\n",
    "  Returns:\n",
    "      list: List of descriptions where index i contains the description for item i\n",
    "  \"\"\"\n",
    "  descriptions = []\n",
    "  with open(\"/content/descriptions.txt\", \"r\") as f:\n",
    "      for line in f:\n",
    "          idx, desc = line.strip().split(',', 1)\n",
    "          # Make sure list is long enough\n",
    "          while len(descriptions) <= int(idx):\n",
    "              descriptions.append(None)\n",
    "          descriptions[int(idx)] = desc\n",
    "  return descriptions\n",
    "\n",
    "def generate_uniform_hypersphere(m, n):\n",
    "    \"\"\"\n",
    "    Generate m points uniformly distributed on an n-dimensional unit hypersphere.\n",
    "\n",
    "    This implementation uses the fact that normalizing vectors sampled from a\n",
    "    multivariate normal distribution results in uniform distribution on the sphere.\n",
    "\n",
    "    Parameters:\n",
    "    m (int): Number of vectors to generate\n",
    "    n (int): Dimension of the space\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array of shape (m, n) containing m n-dimensional unit vectors\n",
    "    \"\"\"\n",
    "    # Generate random vectors from standard normal distribution\n",
    "    vectors = np.random.normal(0, 1, (m, n))\n",
    "\n",
    "    # Normalize each vector to lie on unit hypersphere\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    uniform_vectors = vectors / norms\n",
    "\n",
    "    return torch.tensor(uniform_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR7KSoKarJ16",
    "outputId": "bf84d72a-a1ee-47a1-ca5d-44b49ce4a506"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRp7oXqdsqP5",
    "outputId": "03b213af-04bd-43bd-b9bc-1776a923659c"
   },
   "outputs": [],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "97d6b834470341f4ad85540512d91c5c",
      "9b1bd325aa76441d8798501f9d112df4",
      "725d2c892fc4430c87708ab618d65f6f",
      "3e0f48fa9beb4e79b8be730921ebe084",
      "e5c4621e2aaa4c7ab53dc6dee60094ad",
      "7956ed9fa9994dbea617be7593b82fdc",
      "5a37fd23184d40f1a1051a243e3d5be4",
      "bbdd414950744d1899707041883670b2",
      "05d103d26ffc4188905fbe98414f5045",
      "e0687cbe6190401d9bab544efcc6a73f",
      "46ff2deddb254720bb5355e81747f33c"
     ]
    },
    "id": "1IbLDU3Vll1g",
    "outputId": "406db947-431a-4869-d791-42087735aa2d"
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDHedqgz060X",
    "outputId": "14c3233e-6f96-4855-8e89-b0d235d28ab9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_test_set():\n",
    "  \"\"\"\n",
    "  Test set should be created with corresponding image/audio pairs in /content/{image,audio}_i.{jpg,wav}.\n",
    "\n",
    "  Intermediate language descriptions for PHI_B should be found at /content/descriptions.txt\n",
    "  \"\"\"\n",
    "  descriptions = load_descriptions()\n",
    "\n",
    "  # Find which indices actually have both image and audio files\n",
    "  valid_indices = []\n",
    "  for i in range(len(descriptions)):\n",
    "      image_path = f'/content/image_{i}.jpg'\n",
    "      audio_path = f'/content/audio_{i}.wav'\n",
    "      if os.path.exists(image_path) and os.path.exists(audio_path):\n",
    "          valid_indices.append(i)\n",
    "\n",
    "  # Shuffle the valid indices\n",
    "  shuffled_valid_indices = np.random.permutation(valid_indices)\n",
    "\n",
    "  # Take only N items (or all if less than N available)\n",
    "  N = 25\n",
    "  n_available = min(N, len(shuffled_valid_indices))\n",
    "  selected_indices = shuffled_valid_indices[:n_available]\n",
    "\n",
    "  # Create the path lists using only valid indices\n",
    "  image_paths = [f'/content/image_{i}.jpg' for i in selected_indices]\n",
    "  audio_paths = [f'/content/audio_{i}.wav' for i in selected_indices]\n",
    "  text_list = [descriptions[i] for i in selected_indices]\n",
    "\n",
    "  print(f\"Found {len(valid_indices)} valid pairs, using {n_available}\")\n",
    "\n",
    "  return image_paths, audio_paths, text_list\n",
    "\n",
    "image_paths, audio_paths, text_list = create_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZdWQWcjkSXK",
    "outputId": "a52b2622-92ac-43d1-a652-28309e76c5fd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def run_timing_test():\n",
    "    # Time file existence check\n",
    "    start_time = time.time()\n",
    "    valid_indices = []\n",
    "    for i in range(1):\n",
    "        image_path = f'/content/image_{i}.jpg'\n",
    "        if os.path.exists(image_path):\n",
    "            valid_indices.append(i)\n",
    "    file_check_time = time.time() - start_time\n",
    "\n",
    "    # Time path list creation\n",
    "    start_time = time.time()\n",
    "    image_paths = [f'/content/image_{i}.jpg' for i in valid_indices]\n",
    "    path_creation_time = time.time() - start_time\n",
    "\n",
    "    # Time data loading and model inference\n",
    "    start_time = time.time()\n",
    "    inputs = {\n",
    "        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    }\n",
    "    data_load_time = time.time() - start_time\n",
    "\n",
    "    # Time model inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Print results\n",
    "    print(f\"File existence check time: {file_check_time:.3f} seconds\")\n",
    "    print(f\"Path list creation time: {path_creation_time:.3f} seconds\")\n",
    "    print(f\"Data loading time: {data_load_time:.3f} seconds\")\n",
    "    print(f\"Model inference time: {inference_time:.3f} seconds\")\n",
    "    print(f\"Total time: {file_check_time + path_creation_time + data_load_time + inference_time:.3f} seconds\")\n",
    "    print(f\"Number of valid images: {len(valid_indices)}\")\n",
    "\n",
    "# Run the test\n",
    "run_timing_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uT7PYBdXlHsX",
    "outputId": "29e40378-060d-4682-d9ec-1c2ff5637585"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def time_dot_product(n_repeats=1000):\n",
    "    # Create two random 512-dim vectors\n",
    "    v1 = torch.randn(512, device=device)\n",
    "    v2 = torch.randn(512, device=device)\n",
    "\n",
    "    # Time dot product\n",
    "    start_time = time.time()\n",
    "    for _ in range(n_repeats):\n",
    "        with torch.no_grad():\n",
    "            dot = torch.dot(v1, v2)\n",
    "    dot_time = (time.time() - start_time) / n_repeats\n",
    "\n",
    "    print(f\"Average dot product time: {dot_time*1000:.3f} milliseconds\")  # Convert to ms\n",
    "    print(f\"Average dot product time: {dot_time*1_000_000:.3f} microseconds\")  # Convert to μs\n",
    "    return dot_time\n",
    "\n",
    "def run_timing_test():\n",
    "    # Time file existence check\n",
    "    start_time = time.time()\n",
    "    valid_indices = []\n",
    "    for i in range(1):\n",
    "        image_path = f'/content/image_{i}.jpg'\n",
    "        if os.path.exists(image_path):\n",
    "            valid_indices.append(i)\n",
    "    file_check_time = time.time() - start_time\n",
    "\n",
    "    # Time path list creation\n",
    "    start_time = time.time()\n",
    "    image_paths = [f'/content/image_{i}.jpg' for i in valid_indices]\n",
    "    path_creation_time = time.time() - start_time\n",
    "\n",
    "    # Time data loading and model inference\n",
    "    start_time = time.time()\n",
    "    inputs = {\n",
    "        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    }\n",
    "    data_load_time = time.time() - start_time\n",
    "\n",
    "    # Time model inference\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nMain Pipeline Timing:\")\n",
    "    print(f\"File existence check time: {file_check_time:.3f} seconds\")\n",
    "    print(f\"Path list creation time: {path_creation_time:.3f} seconds\")\n",
    "    print(f\"Data loading time: {data_load_time:.3f} seconds\")\n",
    "    print(f\"Model inference time: {inference_time:.3f} seconds\")\n",
    "    print(f\"Total time: {file_check_time + path_creation_time + data_load_time + inference_time:.3f} seconds\")\n",
    "    print(f\"Number of valid images: {len(valid_indices)}\")\n",
    "\n",
    "print(\"Timing dot product...\")\n",
    "dot_time = time_dot_product()\n",
    "print(\"\\nRunning main pipeline timing...\")\n",
    "run_timing_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "WF1D6sbq02CI",
    "outputId": "b0a7281f-1dae-4955-8893-b51201dea97c"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_embeddings(image_paths, audio_paths, text_list):\n",
    "  # Load data\n",
    "  inputs = {\n",
    "      ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "      ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "      ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "  }\n",
    "\n",
    "  with torch.no_grad():\n",
    "      embeddings = model(inputs)\n",
    "\n",
    "  return embeddings\n",
    "\n",
    "embeddings = get_embeddings(image_paths, audio_paths, text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MO6YwkoWlpD5",
    "outputId": "c76c845b-7e51-4fc7-d8d9-3f130f934bb6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "embeddings_A = embeddings[ModalityType.AUDIO].cpu()\n",
    "embeddings_C = embeddings[ModalityType.TEXT].cpu()\n",
    "\n",
    "print(embeddings_A.shape)\n",
    "print(embeddings_C.shape)\n",
    "\n",
    "# plt.imshow(grid)\n",
    "# plt.show()\n",
    "\n",
    "mc_sizes = [100, 500, 1000, 2500, 5000]\n",
    "tau = 5\n",
    "\n",
    "for M in mc_sizes:\n",
    "  PHI_B = generate_uniform_hypersphere(M, embeddings_A.shape[1])\n",
    "\n",
    "  grid_lse = np.zeros(grid.shape)\n",
    "  for i in range(embeddings_A.shape[0]):\n",
    "    for j in range(embeddings_C.shape[0]):\n",
    "      for phi in PHI_B:\n",
    "        grid_lse[i, j] += torch.exp(embeddings_A[i] @ phi.T.float() / tau + phi.float() @ embeddings_C[j].T / tau).detach().cpu().numpy()\n",
    "\n",
    "  acc_lse = get_row_diagonal_accuracy(grid_lse)\n",
    "  print(f'Accuracy for M = {M}: {acc_lse}')\n",
    "  plt.imshow(grid_lse)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7V68xU-vqDGi",
    "outputId": "b3188c68-7b6b-4bf3-a916-2465b3dae348"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def generate_uniform_hypersphere(M, dim):\n",
    "    \"\"\"Generate points on unit hypersphere more efficiently\"\"\"\n",
    "    # Generate random normal vectors\n",
    "    vectors = torch.randn(M, dim)\n",
    "    # Normalize to unit length\n",
    "    return vectors / torch.norm(vectors, dim=1, keepdim=True)\n",
    "\n",
    "def compute_grid_lse_batched(embeddings_A, embeddings_C, PHI_B, tau=5, batch_size=100):\n",
    "    \"\"\"\n",
    "    Compute LSE grid using batched operations on GPU\n",
    "    \"\"\"\n",
    "    M = PHI_B.shape[0]\n",
    "    N_A = embeddings_A.shape[0]\n",
    "    N_C = embeddings_C.shape[0]\n",
    "    device = embeddings_A.device\n",
    "\n",
    "    # Move PHI_B to GPU if not already there\n",
    "    PHI_B = PHI_B.to(device)\n",
    "\n",
    "    # Initialize output grid on GPU\n",
    "    grid_lse = torch.zeros((N_A, N_C), device=device)\n",
    "\n",
    "    # Process in batches of PHI_B to manage memory\n",
    "    for phi_start in range(0, M, batch_size):\n",
    "        phi_end = min(phi_start + batch_size, M)\n",
    "        phi_batch = PHI_B[phi_start:phi_end]\n",
    "\n",
    "        # Compute first term: embeddings_A @ phi.T\n",
    "        term1 = embeddings_A @ phi_batch.T  # [N_A, batch_size]\n",
    "\n",
    "        # Compute second term: phi @ embeddings_C.T\n",
    "        term2 = phi_batch @ embeddings_C.T  # [batch_size, N_C]\n",
    "\n",
    "        # Broadcasting will handle the addition\n",
    "        # [N_A, batch_size, 1] + [batch_size, N_C] -> [N_A, batch_size, N_C]\n",
    "        combined = (term1.unsqueeze(-1) + term2.unsqueeze(0)) / tau\n",
    "\n",
    "        # Sum the exponentials for this batch\n",
    "        grid_lse += torch.exp(combined).sum(dim=1)\n",
    "\n",
    "    return grid_lse.detach()\n",
    "\n",
    "def get_row_diagonal_accuracy(grid):\n",
    "    \"\"\"Calculate accuracy from grid\"\"\"\n",
    "    predictions = torch.argmax(grid, dim=1)\n",
    "    correct = (predictions == torch.arange(len(predictions), device=predictions.device))\n",
    "    return correct.float().mean().item()\n",
    "\n",
    "# Main execution\n",
    "def run_mc_analysis(embeddings_A, embeddings_C, mc_sizes, tau=5):\n",
    "    \"\"\"Run Monte Carlo analysis for different sample sizes\"\"\"\n",
    "    device = embeddings_A.device\n",
    "    results = []\n",
    "    grids = []\n",
    "\n",
    "    # Compute reference grid\n",
    "    ref_grid = torch.softmax(embeddings_A @ embeddings_C.T, dim=-1)\n",
    "    ref_acc = get_row_diagonal_accuracy(ref_grid)\n",
    "    print(f'Reference accuracy: {ref_acc:.4f}')\n",
    "\n",
    "    for M in mc_sizes:\n",
    "        # Generate points on hypersphere (on CPU to save GPU memory)\n",
    "        PHI_B = generate_uniform_hypersphere(M, embeddings_A.shape[1])\n",
    "\n",
    "        # Compute LSE grid\n",
    "        grid_lse = compute_grid_lse_batched(embeddings_A, embeddings_C, PHI_B, tau)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc_lse = get_row_diagonal_accuracy(grid_lse)\n",
    "        results.append(acc_lse)\n",
    "        grids.append(grid_lse.cpu().numpy())\n",
    "\n",
    "        print(f'Accuracy for M = {M}: {acc_lse:.4f}')\n",
    "\n",
    "    return results, grids\n",
    "\n",
    "# Example usage:\n",
    "mc_sizes = [2**i for i in range(17)]\n",
    "results, grids = run_mc_analysis(embeddings_A, embeddings_C, mc_sizes, tau=5)\n",
    "\n",
    "# Plot results\n",
    "for grid in grids:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "a6kA9v9-ziK4",
    "outputId": "59e242c8-561d-4369-e366-082517e13c5b"
   },
   "outputs": [],
   "source": [
    "# After running the analysis\n",
    "mc_sizes = [2**i for i in range(18)]\n",
    "results, grids = run_mc_analysis(embeddings_A, embeddings_C, mc_sizes, tau=5)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot MC results\n",
    "plt.semilogx(mc_sizes, results, 'b.-', label='Monte Carlo Estimation')\n",
    "\n",
    "# Add reference accuracy line\n",
    "ref_acc = get_row_diagonal_accuracy(torch.softmax(embeddings_A @ embeddings_C.T, dim=-1))\n",
    "plt.axhline(y=ref_acc, color='r', linestyle='--', label='Direct Computation')\n",
    "\n",
    "# Customize the plot\n",
    "plt.grid(True)\n",
    "plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Monte Carlo Samples')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 916
    },
    "id": "u-pITtcj0pTU",
    "outputId": "910df764-c568-4bd8-9ab8-93e280b88ba9"
   },
   "outputs": [],
   "source": [
    "NUM_TRIALS = 5\n",
    "\n",
    "for i in range(NUM_TRIALS)\n",
    "  image_paths, audio_paths, text_list = create_test_set()\n",
    "  embeddings = get_embeddings(image_paths, audio_paths, text_list)\n",
    "\n",
    "  embeddings_A = embeddings[ModalityType.AUDIO].cpu()\n",
    "  embeddings_C = embeddings[ModalityType.TEXT].cpu()\n",
    "\n",
    "  # After running the analysis\n",
    "  mc_sizes = [2**i for i in range(18)]\n",
    "  results, grids = run_mc_analysis(embeddings_A, embeddings_C, mc_sizes, tau=5)\n",
    "\n",
    "  # Create the plot\n",
    "  plt.figure(figsize=(10, 6))\n",
    "\n",
    "  # Plot MC results\n",
    "  plt.semilogx(mc_sizes, results, 'b.-', label='Monte Carlo Estimation')\n",
    "\n",
    "  # Add reference accuracy line\n",
    "  ref_acc = get_row_diagonal_accuracy(torch.softmax(embeddings_A @ embeddings_C.T, dim=-1))\n",
    "  plt.axhline(y=ref_acc, color='r', linestyle='--', label='Direct Computation')\n",
    "\n",
    "  # Customize the plot\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Accuracy vs Number of Monte Carlo Samples')\n",
    "  plt.legend()\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdpXr8jc2x62",
    "outputId": "0f1f25de-3967-43d9-98ef-bfd448741676"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "NUM_TRIALS = 50\n",
    "mc_sizes = [2**i for i in range(20)]\n",
    "\n",
    "# Arrays to store results\n",
    "all_results = np.zeros((NUM_TRIALS, len(mc_sizes)))\n",
    "all_refs = np.zeros(NUM_TRIALS)\n",
    "\n",
    "# Run trials\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(f\"Running trial {i+1}/{NUM_TRIALS}\")\n",
    "\n",
    "    # Get new dataset\n",
    "    image_paths, audio_paths, text_list = create_test_set()\n",
    "    embeddings = get_embeddings(image_paths, audio_paths, text_list)\n",
    "\n",
    "    embeddings_A = embeddings[ModalityType.AUDIO].cpu()\n",
    "    embeddings_C = embeddings[ModalityType.TEXT].cpu()\n",
    "\n",
    "    # Run analysis\n",
    "    results, _ = run_mc_analysis(embeddings_A, embeddings_C, mc_sizes, tau=5)\n",
    "    ref_acc = get_row_diagonal_accuracy(torch.softmax(embeddings_A @ embeddings_C.T, dim=-1))\n",
    "\n",
    "    # Store results\n",
    "    all_results[i] = results\n",
    "    all_refs[i] = ref_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "id": "uNszn3rr2yWF",
    "outputId": "4856f9bd-c9c2-49e5-c990-46a36da88ce7"
   },
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "mean_results = np.mean(all_results, axis=0)\n",
    "mean_ref = np.mean(all_refs)\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "confidence_intervals = stats.sem(all_results, axis=0) * stats.t.ppf((1 + 0.95) / 2, NUM_TRIALS-1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot MC results with error bars\n",
    "plt.errorbar(mc_sizes, mean_results, yerr=confidence_intervals, fmt='b.-',\n",
    "            label='Monte Carlo Estimation', capsize=5)\n",
    "\n",
    "# Add reference accuracy line with shaded error region\n",
    "ref_confidence = stats.sem(all_refs) * stats.t.ppf((1 + 0.95) / 2, NUM_TRIALS-1)\n",
    "plt.axhline(y=mean_ref, color='r', linestyle='--', label='Direct Computation')\n",
    "plt.fill_between(mc_sizes, mean_ref - ref_confidence, mean_ref + ref_confidence,\n",
    "                color='r', alpha=0.2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.grid(True)\n",
    "plt.xscale('log')  # Using xscale instead of semilogx for compatibility with errorbar\n",
    "plt.xlabel('Number of Monte Carlo Samples (M)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(f'Accuracy vs Number of Monte Carlo Samples ({NUM_TRIALS} trials) for ImageBind')\n",
    "plt.legend()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nMean reference accuracy: {mean_ref:.4f} ± {ref_confidence:.4f}\")\n",
    "for size, mean, ci in zip(mc_sizes, mean_results, confidence_intervals):\n",
    "    print(f\"M={size}: {mean:.4f} ± {ci:.4f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "524UZ82E4XMp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05d103d26ffc4188905fbe98414f5045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e0f48fa9beb4e79b8be730921ebe084": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0687cbe6190401d9bab544efcc6a73f",
      "placeholder": "​",
      "style": "IPY_MODEL_46ff2deddb254720bb5355e81747f33c",
      "value": " 4.47G/4.47G [00:37&lt;00:00, 177MB/s]"
     }
    },
    "46ff2deddb254720bb5355e81747f33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a37fd23184d40f1a1051a243e3d5be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "725d2c892fc4430c87708ab618d65f6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbdd414950744d1899707041883670b2",
      "max": 4803584173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_05d103d26ffc4188905fbe98414f5045",
      "value": 4803584173
     }
    },
    "7956ed9fa9994dbea617be7593b82fdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97d6b834470341f4ad85540512d91c5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9b1bd325aa76441d8798501f9d112df4",
       "IPY_MODEL_725d2c892fc4430c87708ab618d65f6f",
       "IPY_MODEL_3e0f48fa9beb4e79b8be730921ebe084"
      ],
      "layout": "IPY_MODEL_e5c4621e2aaa4c7ab53dc6dee60094ad"
     }
    },
    "9b1bd325aa76441d8798501f9d112df4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7956ed9fa9994dbea617be7593b82fdc",
      "placeholder": "​",
      "style": "IPY_MODEL_5a37fd23184d40f1a1051a243e3d5be4",
      "value": "100%"
     }
    },
    "bbdd414950744d1899707041883670b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0687cbe6190401d9bab544efcc6a73f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c4621e2aaa4c7ab53dc6dee60094ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
